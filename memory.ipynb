{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[\n",
       "  HumanMessage {\n",
       "    lc_serializable: \u001b[33mtrue\u001b[39m,\n",
       "    lc_kwargs: { content: \u001b[32m\"hi\"\u001b[39m, additional_kwargs: {}, response_metadata: {} },\n",
       "    lc_namespace: [ \u001b[32m\"langchain_core\"\u001b[39m, \u001b[32m\"messages\"\u001b[39m ],\n",
       "    content: \u001b[32m\"hi\"\u001b[39m,\n",
       "    name: \u001b[90mundefined\u001b[39m,\n",
       "    additional_kwargs: {},\n",
       "    response_metadata: {}\n",
       "  },\n",
       "  AIMessage {\n",
       "    lc_serializable: \u001b[33mtrue\u001b[39m,\n",
       "    lc_kwargs: {\n",
       "      content: \u001b[32m\"whats can I do for you?\"\u001b[39m,\n",
       "      additional_kwargs: {},\n",
       "      response_metadata: {}\n",
       "    },\n",
       "    lc_namespace: [ \u001b[32m\"langchain_core\"\u001b[39m, \u001b[32m\"messages\"\u001b[39m ],\n",
       "    content: \u001b[32m\"whats can I do for you?\"\u001b[39m,\n",
       "    name: \u001b[90mundefined\u001b[39m,\n",
       "    additional_kwargs: {},\n",
       "    response_metadata: {}\n",
       "  }\n",
       "]"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import { ChatMessageHistory } from 'langchain/stores/message/in_memory';\n",
    "import { HumanMessage, AIMessage } from '@langchain/core/messages';\n",
    "const history = new ChatMessageHistory()\n",
    "await history.addMessage(new HumanMessage(\"hi\"))\n",
    "await history.addMessage(new AIMessage(\"whats can I do for you?\"))\n",
    "const message = await history.getMessages()\n",
    "message"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import { ChatPromptTemplate, MessagesPlaceholder } from \"@langchain/core/prompts\";\n",
    "import { Ollama } from \"@langchain/community/llms/ollama\";\n",
    "import { ChatMessageHistory } from 'langchain/stores/message/in_memory';\n",
    "import { HumanMessage, AIMessage } from '@langchain/core/messages';\n",
    "const chatModel = new Ollama({\n",
    "    baseUrl: \"http://localhost:11434\", \n",
    "    model: \"llama3\",\n",
    "});\n",
    "const prompt = ChatPromptTemplate.fromMessages([\n",
    "    [\"system\", `You are a helpful assistant. Answer all questions to the best of your ability.\n",
    "    You are talkative and provides lots of specific details from its context. \n",
    "    If the you does not know the answer to a question, it truthfully says you do not know.`],\n",
    "    new MessagesPlaceholder(\"history_message\"),\n",
    "]);\n",
    "\n",
    "const chain = prompt.pipe(chatModel);\n",
    "const history = new ChatMessageHistory();\n",
    "await history.addUserMessage(new HumanMessage(\"hi, my name is yyl\"));\n",
    "const res1 = await chain.invoke({ history_message: await history.getMessages() });\n",
    "console.log(res1, 'res1');\n",
    "await history.addMessage(res1)\n",
    "await history.addMessage(new HumanMessage(\"what is my name?\"))\n",
    "const res2 = await chain.invoke({\n",
    "    history_message: await history.getMessages()\n",
    "})\n",
    "console.log(res2, 'res2')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Hello Yyl! It's nice to meet you. I'm here to help with any questions or topics you'd like to discuss. What brings you here today? Do you have something specific on your mind that you'd like some assistance with?\n"
     ]
    }
   ],
   "source": [
    "// 自动维护chat history\n",
    "import { ChatPromptTemplate, MessagesPlaceholder } from \"@langchain/core/prompts\";\n",
    "import { Ollama } from \"@langchain/community/llms/ollama\";\n",
    "import { ChatMessageHistory } from 'langchain/stores/message/in_memory';\n",
    "import { HumanMessage, AIMessage } from '@langchain/core/messages';\n",
    "import { RunnableWithMessageHistory } from \"@langchain/core/runnables\";\n",
    "const chatModel = new Ollama({\n",
    "    baseUrl: \"http://localhost:11434\", \n",
    "    model: \"llama3\",\n",
    "});\n",
    "const prompt = ChatPromptTemplate.fromMessages([\n",
    "    [\"system\", \"You are a helpful assistant. Answer all questions to the best of your ability.\"],\n",
    "    new MessagesPlaceholder(\"history_message\"),\n",
    "    [\"human\",\"{input}\"]\n",
    "])\n",
    "const history = new ChatMessageHistory();\n",
    "const chain = prompt.pipe(chatModel)\n",
    "const chainWithHistory = new RunnableWithMessageHistory({\n",
    "    runnable: chain,\n",
    "    getMessageHistory: (_sessionId) => history,\n",
    "    inputMessagesKey: \"input\",\n",
    "    historyMessagesKey: \"history_message\",\n",
    "})\n",
    "const res1 = await chainWithHistory.invoke({\n",
    "    input: \"hi, my name is yyl\"\n",
    "},{\n",
    "    configurable: { sessionId: \"none\" }\n",
    "})\n",
    "console.log(res1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\u001b[32m\"You're asking what your name is again! And my answer remains the same: Your name is indeed Yyl!\"\u001b[39m"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "const res2 = await chainWithHistory.invoke({\n",
    "    input: \"我的名字叫什么？\"\n",
    "}, {\n",
    "    configurable: { sessionId: \"none\" }\n",
    "})\n",
    "res2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[\n",
       "  HumanMessage {\n",
       "    lc_serializable: \u001b[33mtrue\u001b[39m,\n",
       "    lc_kwargs: {\n",
       "      content: \u001b[32m\"hi, my name is yyl\"\u001b[39m,\n",
       "      additional_kwargs: {},\n",
       "      response_metadata: {}\n",
       "    },\n",
       "    lc_namespace: [ \u001b[32m\"langchain_core\"\u001b[39m, \u001b[32m\"messages\"\u001b[39m ],\n",
       "    content: \u001b[32m\"hi, my name is yyl\"\u001b[39m,\n",
       "    name: \u001b[90mundefined\u001b[39m,\n",
       "    additional_kwargs: {},\n",
       "    response_metadata: {}\n",
       "  },\n",
       "  AIMessage {\n",
       "    lc_serializable: \u001b[33mtrue\u001b[39m,\n",
       "    lc_kwargs: {\n",
       "      content: \u001b[32m\"Hello Yyl! It's nice to meet you. I'm here to help with any questions or topics you'd like to discus\"\u001b[39m... 112 more characters,\n",
       "      additional_kwargs: {},\n",
       "      response_metadata: {}\n",
       "    },\n",
       "    lc_namespace: [ \u001b[32m\"langchain_core\"\u001b[39m, \u001b[32m\"messages\"\u001b[39m ],\n",
       "    content: \u001b[32m\"Hello Yyl! It's nice to meet you. I'm here to help with any questions or topics you'd like to discus\"\u001b[39m... 112 more characters,\n",
       "    name: \u001b[90mundefined\u001b[39m,\n",
       "    additional_kwargs: {},\n",
       "    response_metadata: {}\n",
       "  },\n",
       "  HumanMessage {\n",
       "    lc_serializable: \u001b[33mtrue\u001b[39m,\n",
       "    lc_kwargs: {\n",
       "      content: \u001b[32m\"我的名字叫什么？\"\u001b[39m,\n",
       "      additional_kwargs: {},\n",
       "      response_metadata: {}\n",
       "    },\n",
       "    lc_namespace: [ \u001b[32m\"langchain_core\"\u001b[39m, \u001b[32m\"messages\"\u001b[39m ],\n",
       "    content: \u001b[32m\"我的名字叫什么？\"\u001b[39m,\n",
       "    name: \u001b[90mundefined\u001b[39m,\n",
       "    additional_kwargs: {},\n",
       "    response_metadata: {}\n",
       "  },\n",
       "  AIMessage {\n",
       "    lc_serializable: \u001b[33mtrue\u001b[39m,\n",
       "    lc_kwargs: {\n",
       "      content: \u001b[32m\"Nice to meet you too, Yyl!\\n\"\u001b[39m +\n",
       "        \u001b[32m\"\\n\"\u001b[39m +\n",
       "        \u001b[32m`The question \"我的名字叫什么\" is in Chinese, and the translation is \"What's my `\u001b[39m... 54 more characters,\n",
       "      additional_kwargs: {},\n",
       "      response_metadata: {}\n",
       "    },\n",
       "    lc_namespace: [ \u001b[32m\"langchain_core\"\u001b[39m, \u001b[32m\"messages\"\u001b[39m ],\n",
       "    content: \u001b[32m\"Nice to meet you too, Yyl!\\n\"\u001b[39m +\n",
       "      \u001b[32m\"\\n\"\u001b[39m +\n",
       "      \u001b[32m`The question \"我的名字叫什么\" is in Chinese, and the translation is \"What's my `\u001b[39m... 54 more characters,\n",
       "    name: \u001b[90mundefined\u001b[39m,\n",
       "    additional_kwargs: {},\n",
       "    response_metadata: {}\n",
       "  },\n",
       "  HumanMessage {\n",
       "    lc_serializable: \u001b[33mtrue\u001b[39m,\n",
       "    lc_kwargs: {\n",
       "      content: \u001b[32m\"我的名字叫什么？\"\u001b[39m,\n",
       "      additional_kwargs: {},\n",
       "      response_metadata: {}\n",
       "    },\n",
       "    lc_namespace: [ \u001b[32m\"langchain_core\"\u001b[39m, \u001b[32m\"messages\"\u001b[39m ],\n",
       "    content: \u001b[32m\"我的名字叫什么？\"\u001b[39m,\n",
       "    name: \u001b[90mundefined\u001b[39m,\n",
       "    additional_kwargs: {},\n",
       "    response_metadata: {}\n",
       "  },\n",
       "  AIMessage {\n",
       "    lc_serializable: \u001b[33mtrue\u001b[39m,\n",
       "    lc_kwargs: {\n",
       "      content: \u001b[32m\"You're asking what your name is again! And my answer remains the same: Your name is indeed Yyl!\"\u001b[39m,\n",
       "      additional_kwargs: {},\n",
       "      response_metadata: {}\n",
       "    },\n",
       "    lc_namespace: [ \u001b[32m\"langchain_core\"\u001b[39m, \u001b[32m\"messages\"\u001b[39m ],\n",
       "    content: \u001b[32m\"You're asking what your name is again! And my answer remains the same: Your name is indeed Yyl!\"\u001b[39m,\n",
       "    name: \u001b[90mundefined\u001b[39m,\n",
       "    additional_kwargs: {},\n",
       "    response_metadata: {}\n",
       "  }\n",
       "]"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "await history.getMessages()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\u001b[32m\"The conversation has just begun, and the first two pieces of information shared are that the person'\"\u001b[39m... 40 more characters"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import { RunnableSequence } from \"@langchain/core/runnables\"\n",
    "import { StringOutputParser } from \"@langchain/core/output_parsers\"\n",
    "const summaryModel = new Ollama({\n",
    "    baseUrl: \"http://localhost:11434\", \n",
    "    model: \"llama3\",\n",
    "});\n",
    "const summaryPrompt = ChatPromptTemplate.fromTemplate(`\n",
    "Progressively summarize the lines of conversation provided, adding onto the previous summary returning a new summary\n",
    "\n",
    "Current summary:\n",
    "{summary}\n",
    "\n",
    "New lines of conversation:\n",
    "{new_lines}\n",
    "\n",
    "New summary:\n",
    "`);\n",
    "const summaryChain = RunnableSequence.from([\n",
    "    summaryPrompt,\n",
    "    summaryModel,\n",
    "    new StringOutputParser(),\n",
    "])\n",
    "const newSummary = await summaryChain.invoke({\n",
    "    summary: \"\",\n",
    "    new_lines: \"I'm 18\",\n",
    "})\n",
    "await summaryChain.invoke({\n",
    "    summary: newSummary,\n",
    "    new_lines: \"I'm female\"\n",
    "})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import { RunnableMap, RunnablePassthrough } from \"@langchain/core/runnables\";\n",
    "import { getBufferString } from \"@langchain/core/messages\";\n",
    "const chatModel = new Ollama({\n",
    "    baseUrl: \"http://localhost:11434\", \n",
    "    model: \"llama3\",\n",
    "});\n",
    "const chatPrompt = ChatPromptTemplate.fromMessages([\n",
    "    [\"system\", `You are a helpful assistant. Answer all questions to the best of your ability.\n",
    "\n",
    "    Here is the chat history summary:\n",
    "    {history_summary}\n",
    "    `],\n",
    "     [\"human\",\"{input}\"]\n",
    "]);\n",
    "let summary = \"\"\n",
    "const history = new ChatMessageHistory();\n",
    "\n",
    "const chatChain = RunnableSequence.from([\n",
    "    {\n",
    "        input: new RunnablePassthrough({\n",
    "            func: (input) => history.addUserMessage(input)\n",
    "        })\n",
    "    },\n",
    "    RunnablePassthrough.assign({\n",
    "        history_summary: () => summary\n",
    "    }),\n",
    "    chatPrompt,\n",
    "    chatModel,\n",
    "    new StringOutputParser(),\n",
    "    new RunnablePassthrough({\n",
    "        func: async (input) => {\n",
    "            history.addAIChatMessage(input)\n",
    "            const messages = await history.getMessages()\n",
    "            const new_lines = getBufferString(messages)\n",
    "            const newSummary = await summaryChain.invoke({\n",
    "                summary,\n",
    "                new_lines\n",
    "            })\n",
    "            history.clear()\n",
    "            summary = newSummary\n",
    "        }\n",
    "    })\n",
    "])\n",
    "await chatChain.invoke(\"I'm hungry now\")\n",
    "\n",
    "// const mapChain = RunnableMap.from({\n",
    "//     a: () => \"a\",\n",
    "//     b: () => \"b\"\n",
    "// })\n",
    "\n",
    "// const res = await mapChain.invoke()\n",
    "\n",
    "// res"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\u001b[32m\"You're craving instant noodles! That's an easy one to satisfy.\\n\"\u001b[39m +\n",
       "  \u001b[32m\"\\n\"\u001b[39m +\n",
       "  \u001b[32m\"So, you mentioned wanting to eat ins\"\u001b[39m... 380 more characters"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "await chatChain.invoke(\"我今天想吃方便面\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Deno",
   "language": "typescript",
   "name": "deno"
  },
  "language_info": {
   "codemirror_mode": "typescript",
   "file_extension": ".ts",
   "mimetype": "text/x.typescript",
   "name": "typescript",
   "nbconvert_exporter": "script",
   "pygments_lexer": "typescript",
   "version": "5.4.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
