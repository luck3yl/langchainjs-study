{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[\n",
       "  HumanMessage {\n",
       "    lc_serializable: \u001b[33mtrue\u001b[39m,\n",
       "    lc_kwargs: { content: \u001b[32m\"hi\"\u001b[39m, additional_kwargs: {}, response_metadata: {} },\n",
       "    lc_namespace: [ \u001b[32m\"langchain_core\"\u001b[39m, \u001b[32m\"messages\"\u001b[39m ],\n",
       "    content: \u001b[32m\"hi\"\u001b[39m,\n",
       "    name: \u001b[90mundefined\u001b[39m,\n",
       "    additional_kwargs: {},\n",
       "    response_metadata: {}\n",
       "  },\n",
       "  AIMessage {\n",
       "    lc_serializable: \u001b[33mtrue\u001b[39m,\n",
       "    lc_kwargs: {\n",
       "      content: \u001b[32m\"whats can I do for you?\"\u001b[39m,\n",
       "      additional_kwargs: {},\n",
       "      response_metadata: {}\n",
       "    },\n",
       "    lc_namespace: [ \u001b[32m\"langchain_core\"\u001b[39m, \u001b[32m\"messages\"\u001b[39m ],\n",
       "    content: \u001b[32m\"whats can I do for you?\"\u001b[39m,\n",
       "    name: \u001b[90mundefined\u001b[39m,\n",
       "    additional_kwargs: {},\n",
       "    response_metadata: {}\n",
       "  }\n",
       "]"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import { ChatMessageHistory } from 'langchain/stores/message/in_memory';\n",
    "import { HumanMessage, AIMessage } from '@langchain/core/messages';\n",
    "const history = new ChatMessageHistory()\n",
    "await history.addMessage(new HumanMessage(\"hi\"))\n",
    "await history.addMessage(new AIMessage(\"whats can I do for you?\"))\n",
    "const message = await history.getMessages()\n",
    "message"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Nice to meet you, Yyl! I'm happy to help with any questions or topics you'd like to discuss. As we start our conversation, I just want to let you know that I'll be doing my best to provide you with accurate and helpful information.\n",
      "\n",
      "To get started, can you tell me a bit more about what's on your mind today? Do you have a specific question in mind or would you like to chat about something in particular? res1\n"
     ]
    },
    {
     "ename": "InputFormatError",
     "evalue": "Error: Field \"history_message\" in prompt uses a MessagesPlaceholder, which expects an array of BaseMessages as an input value. Received: [\n  {\n    \"lc\": 1,\n    \"type\": \"constructor\",\n    \"id\": [\n      \"langchain_core\",\n      \"messages\",\n      \"HumanMessage\"\n    ],\n    \"kwargs\": {\n      \"lc_serializable\": true,\n      \"lc_kwargs\": {\n        \"lc\": 1,\n        \"type\": \"constructor\",\n        \"id\": [\n          \"langchain_core\",\n          \"messages\",\n          \"HumanMessage\"\n        ],\n        \"kwargs\": {\n          \"content\": \"hi, my name is yyl\",\n          \"additional_kwargs\": {},\n          \"response_metadata\": {}\n        }\n      },\n      \"lc_namespace\": [\n        \"langchain_core\",\n        \"messages\"\n      ],\n      \"content\": \"hi, my name is yyl\",\n      \"additional_kwargs\": {},\n      \"response_metadata\": {}\n    }\n  },\n  \"Nice to meet you, Yyl! I'm happy to help with any questions or topics you'd like to discuss. As we start our conversation, I just want to let you know that I'll be doing my best to provide you with accurate and helpful information.\\n\\nTo get started, can you tell me a bit more about what's on your mind today? Do you have a specific question in mind or would you like to chat about something in particular?\",\n  {\n    \"lc\": 1,\n    \"type\": \"constructor\",\n    \"id\": [\n      \"langchain_core\",\n      \"messages\",\n      \"HumanMessage\"\n    ],\n    \"kwargs\": {\n      \"content\": \"what is my name?\",\n      \"additional_kwargs\": {},\n      \"response_metadata\": {}\n    }\n  }\n]",
     "output_type": "error",
     "traceback": [
      "Stack trace:",
      "InputFormatError: Error: Field \"history_message\" in prompt uses a MessagesPlaceholder, which expects an array of BaseMessages as an input value. Received: [",
      "  {",
      "    \"lc\": 1,",
      "    \"type\": \"constructor\",",
      "    \"id\": [",
      "      \"langchain_core\",",
      "      \"messages\",",
      "      \"HumanMessage\"",
      "    ],",
      "    \"kwargs\": {",
      "      \"lc_serializable\": true,",
      "      \"lc_kwargs\": {",
      "        \"lc\": 1,",
      "        \"type\": \"constructor\",",
      "        \"id\": [",
      "          \"langchain_core\",",
      "          \"messages\",",
      "          \"HumanMessage\"",
      "        ],",
      "        \"kwargs\": {",
      "          \"content\": \"hi, my name is yyl\",",
      "          \"additional_kwargs\": {},",
      "          \"response_metadata\": {}",
      "        }",
      "      },",
      "      \"lc_namespace\": [",
      "        \"langchain_core\",",
      "        \"messages\"",
      "      ],",
      "      \"content\": \"hi, my name is yyl\",",
      "      \"additional_kwargs\": {},",
      "      \"response_metadata\": {}",
      "    }",
      "  },",
      "  \"Nice to meet you, Yyl! I'm happy to help with any questions or topics you'd like to discuss. As we start our conversation, I just want to let you know that I'll be doing my best to provide you with accurate and helpful information.\\n\\nTo get started, can you tell me a bit more about what's on your mind today? Do you have a specific question in mind or would you like to chat about something in particular?\",",
      "  {",
      "    \"lc\": 1,",
      "    \"type\": \"constructor\",",
      "    \"id\": [",
      "      \"langchain_core\",",
      "      \"messages\",",
      "      \"HumanMessage\"",
      "    ],",
      "    \"kwargs\": {",
      "      \"content\": \"what is my name?\",",
      "      \"additional_kwargs\": {},",
      "      \"response_metadata\": {}",
      "    }",
      "  }",
      "]",
      "    at MessagesPlaceholder.validateInputOrThrow (file:///Users/yyl/Library/Caches/deno/npm/registry.npmjs.org/@langchain/core/0.1.48/dist/prompts/chat.js:92:27)",
      "    at MessagesPlaceholder.formatMessages (file:///Users/yyl/Library/Caches/deno/npm/registry.npmjs.org/@langchain/core/0.1.48/dist/prompts/chat.js:99:14)",
      "    at ChatPromptTemplate.formatMessages (file:///Users/yyl/Library/Caches/deno/npm/registry.npmjs.org/@langchain/core/0.1.48/dist/prompts/chat.js:583:53)",
      "    at eventLoopTick (ext:core/01_core.js:168:7)",
      "    at async ChatPromptTemplate.formatPromptValue (file:///Users/yyl/Library/Caches/deno/npm/registry.npmjs.org/@langchain/core/0.1.48/dist/prompts/chat.js:141:32)",
      "    at async ChatPromptTemplate._callWithConfig (file:///Users/yyl/Library/Caches/deno/npm/registry.npmjs.org/@langchain/core/0.1.48/dist/runnables/base.js:187:22)",
      "    at async RunnableSequence.invoke (file:///Users/yyl/Library/Caches/deno/npm/registry.npmjs.org/@langchain/core/0.1.48/dist/runnables/base.js:1035:33)",
      "    at async <anonymous>:27:14"
     ]
    }
   ],
   "source": [
    "import { ChatPromptTemplate, MessagesPlaceholder } from \"@langchain/core/prompts\";\n",
    "import { Ollama } from \"@langchain/community/llms/ollama\";\n",
    "import { ChatMessageHistory } from 'langchain/stores/message/in_memory';\n",
    "import { HumanMessage, AIMessage } from '@langchain/core/messages';\n",
    "const chatModel = new Ollama({\n",
    "    baseUrl: \"http://localhost:11434\", \n",
    "    model: \"llama3\",\n",
    "});\n",
    "const prompt = ChatPromptTemplate.fromMessages([\n",
    "    [\"system\", `You are a helpful assistant. Answer all questions to the best of your ability.\n",
    "    You are talkative and provides lots of specific details from its context. \n",
    "    If the you does not know the answer to a question, it truthfully says you do not know.`],\n",
    "    new MessagesPlaceholder(\"history_message\"),\n",
    "]);\n",
    "\n",
    "const chain = prompt.pipe(chatModel);\n",
    "const history = new ChatMessageHistory();\n",
    "await history.addUserMessage(new HumanMessage(\"hi, my name is yyl\"));\n",
    "const res1 = await chain.invoke({ history_message: await history.getMessages() });\n",
    "console.log(res1, 'res1');\n",
    "await history.addMessage(res1)\n",
    "await history.addMessage(new HumanMessage(\"what is my name?\"))\n",
    "const res2 = await chain.invoke({\n",
    "    history_message: await history.getMessages()\n",
    "})\n",
    "console.log(res2, 'res2')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Hello Yyl! It's nice to meet you. I'm here to help with any questions or topics you'd like to discuss. What brings you here today? Do you have something specific on your mind that you'd like some assistance with?\n"
     ]
    }
   ],
   "source": [
    "// 自动维护chat history\n",
    "import { ChatPromptTemplate, MessagesPlaceholder } from \"@langchain/core/prompts\";\n",
    "import { Ollama } from \"@langchain/community/llms/ollama\";\n",
    "import { ChatMessageHistory } from 'langchain/stores/message/in_memory';\n",
    "import { HumanMessage, AIMessage } from '@langchain/core/messages';\n",
    "import { RunnableWithMessageHistory } from \"@langchain/core/runnables\";\n",
    "const chatModel = new Ollama({\n",
    "    baseUrl: \"http://localhost:11434\", \n",
    "    model: \"llama3\",\n",
    "});\n",
    "const prompt = ChatPromptTemplate.fromMessages([\n",
    "    [\"system\", \"You are a helpful assistant. Answer all questions to the best of your ability.\"],\n",
    "    new MessagesPlaceholder(\"history_message\"),\n",
    "    [\"human\",\"{input}\"]\n",
    "])\n",
    "const history = new ChatMessageHistory();\n",
    "const chain = prompt.pipe(chatModel)\n",
    "const chainWithHistory = new RunnableWithMessageHistory({\n",
    "    runnable: chain,\n",
    "    getMessageHistory: (_sessionId) => history,\n",
    "    inputMessagesKey: \"input\",\n",
    "    historyMessagesKey: \"history_message\",\n",
    "})\n",
    "const res1 = await chainWithHistory.invoke({\n",
    "    input: \"hi, my name is yyl\"\n",
    "},{\n",
    "    configurable: { sessionId: \"none\" }\n",
    "})\n",
    "console.log(res1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\u001b[32m\"You're asking what your name is again! And my answer remains the same: Your name is indeed Yyl!\"\u001b[39m"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "const res2 = await chainWithHistory.invoke({\n",
    "    input: \"我的名字叫什么？\"\n",
    "}, {\n",
    "    configurable: { sessionId: \"none\" }\n",
    "})\n",
    "res2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[\n",
       "  HumanMessage {\n",
       "    lc_serializable: \u001b[33mtrue\u001b[39m,\n",
       "    lc_kwargs: {\n",
       "      content: \u001b[32m\"hi, my name is yyl\"\u001b[39m,\n",
       "      additional_kwargs: {},\n",
       "      response_metadata: {}\n",
       "    },\n",
       "    lc_namespace: [ \u001b[32m\"langchain_core\"\u001b[39m, \u001b[32m\"messages\"\u001b[39m ],\n",
       "    content: \u001b[32m\"hi, my name is yyl\"\u001b[39m,\n",
       "    name: \u001b[90mundefined\u001b[39m,\n",
       "    additional_kwargs: {},\n",
       "    response_metadata: {}\n",
       "  },\n",
       "  AIMessage {\n",
       "    lc_serializable: \u001b[33mtrue\u001b[39m,\n",
       "    lc_kwargs: {\n",
       "      content: \u001b[32m\"Hello Yyl! It's nice to meet you. I'm here to help with any questions or topics you'd like to discus\"\u001b[39m... 112 more characters,\n",
       "      additional_kwargs: {},\n",
       "      response_metadata: {}\n",
       "    },\n",
       "    lc_namespace: [ \u001b[32m\"langchain_core\"\u001b[39m, \u001b[32m\"messages\"\u001b[39m ],\n",
       "    content: \u001b[32m\"Hello Yyl! It's nice to meet you. I'm here to help with any questions or topics you'd like to discus\"\u001b[39m... 112 more characters,\n",
       "    name: \u001b[90mundefined\u001b[39m,\n",
       "    additional_kwargs: {},\n",
       "    response_metadata: {}\n",
       "  },\n",
       "  HumanMessage {\n",
       "    lc_serializable: \u001b[33mtrue\u001b[39m,\n",
       "    lc_kwargs: {\n",
       "      content: \u001b[32m\"我的名字叫什么？\"\u001b[39m,\n",
       "      additional_kwargs: {},\n",
       "      response_metadata: {}\n",
       "    },\n",
       "    lc_namespace: [ \u001b[32m\"langchain_core\"\u001b[39m, \u001b[32m\"messages\"\u001b[39m ],\n",
       "    content: \u001b[32m\"我的名字叫什么？\"\u001b[39m,\n",
       "    name: \u001b[90mundefined\u001b[39m,\n",
       "    additional_kwargs: {},\n",
       "    response_metadata: {}\n",
       "  },\n",
       "  AIMessage {\n",
       "    lc_serializable: \u001b[33mtrue\u001b[39m,\n",
       "    lc_kwargs: {\n",
       "      content: \u001b[32m\"Nice to meet you too, Yyl!\\n\"\u001b[39m +\n",
       "        \u001b[32m\"\\n\"\u001b[39m +\n",
       "        \u001b[32m`The question \"我的名字叫什么\" is in Chinese, and the translation is \"What's my `\u001b[39m... 54 more characters,\n",
       "      additional_kwargs: {},\n",
       "      response_metadata: {}\n",
       "    },\n",
       "    lc_namespace: [ \u001b[32m\"langchain_core\"\u001b[39m, \u001b[32m\"messages\"\u001b[39m ],\n",
       "    content: \u001b[32m\"Nice to meet you too, Yyl!\\n\"\u001b[39m +\n",
       "      \u001b[32m\"\\n\"\u001b[39m +\n",
       "      \u001b[32m`The question \"我的名字叫什么\" is in Chinese, and the translation is \"What's my `\u001b[39m... 54 more characters,\n",
       "    name: \u001b[90mundefined\u001b[39m,\n",
       "    additional_kwargs: {},\n",
       "    response_metadata: {}\n",
       "  },\n",
       "  HumanMessage {\n",
       "    lc_serializable: \u001b[33mtrue\u001b[39m,\n",
       "    lc_kwargs: {\n",
       "      content: \u001b[32m\"我的名字叫什么？\"\u001b[39m,\n",
       "      additional_kwargs: {},\n",
       "      response_metadata: {}\n",
       "    },\n",
       "    lc_namespace: [ \u001b[32m\"langchain_core\"\u001b[39m, \u001b[32m\"messages\"\u001b[39m ],\n",
       "    content: \u001b[32m\"我的名字叫什么？\"\u001b[39m,\n",
       "    name: \u001b[90mundefined\u001b[39m,\n",
       "    additional_kwargs: {},\n",
       "    response_metadata: {}\n",
       "  },\n",
       "  AIMessage {\n",
       "    lc_serializable: \u001b[33mtrue\u001b[39m,\n",
       "    lc_kwargs: {\n",
       "      content: \u001b[32m\"You're asking what your name is again! And my answer remains the same: Your name is indeed Yyl!\"\u001b[39m,\n",
       "      additional_kwargs: {},\n",
       "      response_metadata: {}\n",
       "    },\n",
       "    lc_namespace: [ \u001b[32m\"langchain_core\"\u001b[39m, \u001b[32m\"messages\"\u001b[39m ],\n",
       "    content: \u001b[32m\"You're asking what your name is again! And my answer remains the same: Your name is indeed Yyl!\"\u001b[39m,\n",
       "    name: \u001b[90mundefined\u001b[39m,\n",
       "    additional_kwargs: {},\n",
       "    response_metadata: {}\n",
       "  }\n",
       "]"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "await history.getMessages()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\u001b[32m\"The conversation has just begun, and the first two pieces of information shared are that the person'\"\u001b[39m... 40 more characters"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import { RunnableSequence } from \"@langchain/core/runnables\"\n",
    "import { StringOutputParser } from \"@langchain/core/output_parsers\"\n",
    "const summaryModel = new Ollama({\n",
    "    baseUrl: \"http://localhost:11434\", \n",
    "    model: \"llama3\",\n",
    "});\n",
    "const summaryPrompt = ChatPromptTemplate.fromTemplate(`\n",
    "Progressively summarize the lines of conversation provided, adding onto the previous summary returning a new summary\n",
    "\n",
    "Current summary:\n",
    "{summary}\n",
    "\n",
    "New lines of conversation:\n",
    "{new_lines}\n",
    "\n",
    "New summary:\n",
    "`);\n",
    "const summaryChain = RunnableSequence.from([\n",
    "    summaryPrompt,\n",
    "    summaryModel,\n",
    "    new StringOutputParser(),\n",
    "])\n",
    "const newSummary = await summaryChain.invoke({\n",
    "    summary: \"\",\n",
    "    new_lines: \"I'm 18\",\n",
    "})\n",
    "await summaryChain.invoke({\n",
    "    summary: newSummary,\n",
    "    new_lines: \"I'm female\"\n",
    "})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\u001b[32m\"You're feeling a bit peckish, eh? What kind of food are you in the mood for? Do you have any craving\"\u001b[39m... 48 more characters"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import { RunnableMap, RunnablePassthrough } from \"@langchain/core/runnables\";\n",
    "import { getBufferString } from \"@langchain/core/messages\";\n",
    "const chatModel = new Ollama({\n",
    "    baseUrl: \"http://localhost:11434\", \n",
    "    model: \"llama3\",\n",
    "});\n",
    "const chatPrompt = ChatPromptTemplate.fromMessages([\n",
    "    [\"system\", `You are a helpful assistant. Answer all questions to the best of your ability.\n",
    "\n",
    "    Here is the chat history summary:\n",
    "    {history_summary}\n",
    "    `],\n",
    "     [\"human\",\"{input}\"]\n",
    "]);\n",
    "let summary = \"\"\n",
    "const history = new ChatMessageHistory();\n",
    "\n",
    "const chatChain = RunnableSequence.from([\n",
    "    {\n",
    "        input: new RunnablePassthrough({\n",
    "            func: (input) => history.addUserMessage(input)\n",
    "        })\n",
    "    },\n",
    "    RunnablePassthrough.assign({\n",
    "        history_summary: () => summary\n",
    "    }),\n",
    "    chatPrompt,\n",
    "    chatModel,\n",
    "    new StringOutputParser(),\n",
    "    new RunnablePassthrough({\n",
    "        func: async (input) => {\n",
    "            history.addAIChatMessage(input)\n",
    "            const messages = await history.getMessages()\n",
    "            const new_lines = getBufferString(messages)\n",
    "            const newSummary = await summaryChain.invoke({\n",
    "                summary,\n",
    "                new_lines\n",
    "            })\n",
    "            history.clear()\n",
    "            summary = newSummary\n",
    "        }\n",
    "    })\n",
    "])\n",
    "await chatChain.invoke(\"I'm hungry now\")\n",
    "\n",
    "// const mapChain = RunnableMap.from({\n",
    "//     a: () => \"a\",\n",
    "//     b: () => \"b\"\n",
    "// })\n",
    "\n",
    "// const res = await mapChain.invoke()\n",
    "\n",
    "// res"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\u001b[32m\"You're craving instant noodles! That's an easy one to satisfy.\\n\"\u001b[39m +\n",
       "  \u001b[32m\"\\n\"\u001b[39m +\n",
       "  \u001b[32m\"So, you mentioned wanting to eat ins\"\u001b[39m... 380 more characters"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "await chatChain.invoke(\"我今天想吃方便面\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Deno",
   "language": "typescript",
   "name": "deno"
  },
  "language_info": {
   "codemirror_mode": "typescript",
   "file_extension": ".ts",
   "mimetype": "text/x.typescript",
   "name": "typescript",
   "nbconvert_exporter": "script",
   "pygments_lexer": "typescript",
   "version": "5.4.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
